{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6805553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import Dict, List, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf8ccf",
   "metadata": {},
   "source": [
    "## SBOM Generation\n",
    "\n",
    "- Step-1: Install Syft\n",
    "> curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh \\\n",
    "| sudo sh -s - -b /usr/local/bin\n",
    "\n",
    "- Step-2: Setup LLM \n",
    "- Step-3: Get the LLM path \n",
    "> python -c \"import sys; print (sys-prefix)\"\n",
    "\n",
    "- Step-4: Generate SBOM in cyclonedx format using below command\n",
    "> syft dir:\"/Users/pgeesala/Desktop/Projects/AI Ethics- Op Sec/-venv\" \\\n",
    "-o cyclonedx-json=sbom_llama_venv.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe08f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results for comparison\n",
    "sbom_results = {}\n",
    "\n",
    "for sbom_file in SBOM_FILES:\n",
    "    sbom_path = Path(sbom_file)\n",
    "    \n",
    "    if not sbom_path.exists():\n",
    "        print(f\"⚠ Skipping {sbom_path.name} - file not found\")\n",
    "        continue\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Analyzing: {sbom_path.name}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load SBOM\n",
    "    result = load_and_extract_sbom(sbom_path)\n",
    "    sbom = result['sbom']\n",
    "    components = result['components']\n",
    "    \n",
    "    print(f\"SBOM Format: {sbom.get('bomFormat')}\")\n",
    "    print(f\"Spec Version: {sbom.get('specVersion')}\")\n",
    "    print(f\"Serial Number: {sbom.get('serialNumber')}\")\n",
    "    print(f\"\\nMetadata:\")\n",
    "    print(f\"  Timestamp: {sbom['metadata']['timestamp']}\")\n",
    "    if 'component' in sbom['metadata']:\n",
    "        print(f\"  Component: {sbom['metadata']['component'].get('name', 'N/A')}\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df_deps = pd.DataFrame(result['deps_data'])\n",
    "    python_packages = df_deps[df_deps['purl'].str.startswith('pkg:pypi', na=False)].copy()\n",
    "    \n",
    "    print(f\"\\nTotal components: {len(components)}\")\n",
    "    print(f\"Python packages: {len(python_packages)}\")\n",
    "    print(f\"Binary components: {len(df_deps[df_deps['type'] == 'application'])}\")\n",
    "    \n",
    "    # Check vulnerabilities\n",
    "    vulnerabilities = check_vulnerabilities(python_packages)\n",
    "    \n",
    "    if vulnerabilities:\n",
    "        print(f\"\\n⚠️  VULNERABILITIES FOUND: {len(vulnerabilities)}\")\n",
    "        for vuln in vulnerabilities:\n",
    "            print(f\"  - {vuln['package']} {vuln['version']}: {vuln['risk']}\")\n",
    "    else:\n",
    "        print(\"\\n✓ No known vulnerable versions detected\")\n",
    "    \n",
    "    # License distribution\n",
    "    print(f\"\\nLicense Distribution (Top 5):\")\n",
    "    print(python_packages['licenses'].value_counts().head(5))\n",
    "    \n",
    "    # Sample packages\n",
    "    print(f\"\\nSample Python Packages (First 10):\")\n",
    "    display(python_packages[['name', 'version', 'licenses']].head(10))\n",
    "    \n",
    "    # Store results\n",
    "    sbom_results[sbom_path.stem] = {\n",
    "        'sbom': sbom,\n",
    "        'df_deps': df_deps,\n",
    "        'python_packages': python_packages,\n",
    "        'vulnerabilities': vulnerabilities\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f004f7",
   "metadata": {},
   "source": [
    "## Download required documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eb3408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os\n",
    "\n",
    "os.makedirs(\"docs\", exist_ok=True)\n",
    "\n",
    "URLS = {\n",
    "    \"deepseek_r1.html\": \"https://huggingface.co/deepseek-ai/DeepSeek-R1\",\n",
    "    \"llama3_8b_instruct.html\": \"https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"gpt4o_system_card.pdf\": \"https://cdn.openai.com/gpt-4o-system-card.pdf\",\n",
    "    \"o3_o4mini_system_card.pdf\": \"https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf\",\n",
    "    \"nist_deepseek_evaluation.pdf\": \"https://www.nist.gov/system/files/documents/2025/09/30/CAISI_Evaluation_of_DeepSeek_AI_Models.pdf\"\n",
    "}\n",
    "\n",
    "for fname, url in URLS.items():\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    with open(os.path.join(\"docs\", fname), \"wb\") as f:\n",
    "        f.write(resp.content)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
