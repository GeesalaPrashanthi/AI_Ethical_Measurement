{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "37112d40",
            "metadata": {},
            "source": [
                "# L4-06: Supply Chain Security Evaluation\n",
                "\n",
                "This notebook analyzes Software Bill of Materials (SBOM) files for our ML models to evaluate supply chain security posture. We're using the CycloneDX format to assess dependency coverage, version fidelity, integrity, and traceability.\n",
                "\n",
                "**What we're evaluating:**\n",
                "- Software dependencies and model artifacts\n",
                "- Version pinning and reproducibility\n",
                "- Component provenance and integrity\n",
                "- Dependency graph completeness"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6c959f96",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import all the dependencies we need\n",
                "from __future__ import annotations\n",
                "from dataclasses import dataclass, asdict\n",
                "from pathlib import Path\n",
                "from typing import Dict, Any, List, Optional, Tuple\n",
                "import json\n",
                "import statistics\n",
                "from datetime import datetime"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5be855bc",
            "metadata": {},
            "source": [
                "## Data Models\n",
                "\n",
                "First, let's define the data structures to hold our metrics. Each category has specific measurements we track."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b89f4fe7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dependency Coverage - tracks what dependencies are documented\n",
                "@dataclass\n",
                "class DependencyCoverageMetrics:\n",
                "    software_dependency_coverage: float   # 0–1, how many software deps are listed\n",
                "    model_artifact_coverage: float        # 0–1, how many ML models are tracked\n",
                "    dataset_coverage: float               # 0–1, datasets used\n",
                "    plugin_tool_coverage: float           # 0–1, plugins/tools coverage\n",
                "\n",
                "# Version Fidelity - are versions pinned properly?\n",
                "@dataclass\n",
                "class VersionFidelityMetrics:\n",
                "    version_specificity: float            # % of components with pinned versions\n",
                "    hash_completeness: float              # % of components with integrity hashes\n",
                "    env_reproducibility: float            # composite score for environment reproducibility\n",
                "\n",
                "# Integrity & Provenance - can we trust the components?\n",
                "@dataclass\n",
                "class IntegrityProvenanceMetrics:\n",
                "    provenance_completeness: float        # do we know where components came from?\n",
                "    signature_validation_rate: float      # how many are cryptographically verified?\n",
                "    unauthorized_component_count: int     # red flag: unknown suppliers\n",
                "\n",
                "# Impact Traceability - understanding dependency relationships\n",
                "@dataclass\n",
                "class ImpactTraceabilityMetrics:\n",
                "    dep_graph_completeness: float         # is the dependency graph complete?\n",
                "    impact_analysis_time_minutes: Optional[float]  # how long to analyze impact\n",
                "    avg_blast_radius: float               # average downstream dependents per component\n",
                "\n",
                "# Top-level metrics container\n",
                "@dataclass\n",
                "class SupplyChainL4Metrics:\n",
                "    dependency_coverage: DependencyCoverageMetrics\n",
                "    version_fidelity: VersionFidelityMetrics\n",
                "    integrity_provenance: IntegrityProvenanceMetrics\n",
                "    impact_traceability: ImpactTraceabilityMetrics"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "83d4e38e",
            "metadata": {},
            "source": [
                "## SBOM Loading Utilities\n",
                "\n",
                "Helper functions to load and parse CycloneDX SBOM files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f974582d",
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_cyclonedx_sbom(path: Path) -> Dict[str, Any]:\n",
                "    \"\"\"Load a CycloneDX SBOM JSON file\"\"\"\n",
                "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
                "        return json.load(f)\n",
                "\n",
                "def get_components(sbom: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
                "    \"\"\"Extract components list from SBOM\"\"\"\n",
                "    return sbom.get(\"components\", [])\n",
                "\n",
                "def get_dependencies_map(sbom: Dict[str, Any]) -> Dict[str, List[str]]:\n",
                "    \"\"\"\n",
                "    Build a dependency map from the SBOM.\n",
                "    Returns mapping of bom-ref -> [list of dependent bom-refs]\n",
                "    \"\"\"\n",
                "    dep_map: Dict[str, List[str]] = {}\n",
                "    for dep in sbom.get(\"dependencies\", []):\n",
                "        ref = dep.get(\"ref\")\n",
                "        depends_on = dep.get(\"dependsOn\", []) or []\n",
                "        dep_map[ref] = depends_on\n",
                "    return dep_map"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6579b0c4",
            "metadata": {},
            "source": [
                "## 1. Dependency Coverage Analysis\n",
                "\n",
                "This checks how comprehensively we've documented our dependencies. We look for:\n",
                "- Software libraries and frameworks\n",
                "- ML model artifacts\n",
                "- Datasets\n",
                "- Plugins and tools"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2830ec92",
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_dependency_coverage(\n",
                "    sbom: Dict[str, Any],\n",
                "    expected_models: Optional[List[str]] = None,\n",
                "    expected_datasets: Optional[List[str]] = None,\n",
                "    expected_plugins: Optional[List[str]] = None,\n",
                "    expected_software_components: Optional[List[str]] = None,\n",
                ") -> DependencyCoverageMetrics:\n",
                "    \"\"\"\n",
                "    Calculate how well our SBOM covers different dependency types.\n",
                "    If you don't provide expected lists, we assume coverage is good if we find at least one.\n",
                "    \"\"\"\n",
                "    components = get_components(sbom)\n",
                "\n",
                "    # Categorize components by type\n",
                "    software = [c for c in components if c.get(\"type\") in (\"library\", \"framework\", \"application\")]\n",
                "    \n",
                "    # Models might be tagged differently, so we check both type and properties\n",
                "    models = [c for c in components if \"model\" in (c.get(\"type\") or \"\").lower()\n",
                "              or \"ml_model\" in \"\".join(p.get(\"value\",\"\") for p in c.get(\"properties\", []))]\n",
                "    \n",
                "    datasets = [c for c in components if \"dataset\" in (c.get(\"type\") or \"\").lower()\n",
                "                or \"dataset\" in (c.get(\"name\") or \"\").lower()]\n",
                "    \n",
                "    plugins = [c for c in components if \"plugin\" in (c.get(\"type\") or \"\").lower()\n",
                "               or \"connector\" in (c.get(\"name\") or \"\").lower()]\n",
                "\n",
                "    def coverage(actual_ids: List[str], expected_ids: Optional[List[str]]) -> float:\n",
                "        \"\"\"Calculate coverage percentage\"\"\"\n",
                "        if expected_ids is None:\n",
                "            return 1.0 if actual_ids else 0.0  # Default to full coverage if anything found\n",
                "        if not expected_ids:\n",
                "            return 1.0\n",
                "\n",
                "        covered = 0\n",
                "        for exp in expected_ids:\n",
                "            # Fuzzy match - case insensitive substring\n",
                "            if any(exp.lower() in (a.lower()) for a in actual_ids):\n",
                "                covered += 1\n",
                "        return covered / len(expected_ids)\n",
                "\n",
                "    # Calculate coverage for each category\n",
                "    software_cov = coverage([c.get(\"name\", \"\") for c in software],\n",
                "                            expected_software_components)\n",
                "    model_cov = coverage([c.get(\"name\", \"\") for c in models],\n",
                "                         expected_models)\n",
                "    dataset_cov = coverage([c.get(\"name\", \"\") for c in datasets],\n",
                "                           expected_datasets)\n",
                "    plugin_cov = coverage([c.get(\"name\", \"\") for c in plugins],\n",
                "                          expected_plugins)\n",
                "\n",
                "    return DependencyCoverageMetrics(\n",
                "        software_dependency_coverage=software_cov,\n",
                "        model_artifact_coverage=model_cov,\n",
                "        dataset_coverage=dataset_cov,\n",
                "        plugin_tool_coverage=plugin_cov,\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1850b07c",
            "metadata": {},
            "source": [
                "## 2. Version Fidelity Analysis\n",
                "\n",
                "Checks if versions are properly pinned (not using ranges like ~1.0 or ^2.0) and if integrity hashes are present. This is crucial for reproducible builds."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7d0c938d",
            "metadata": {},
            "outputs": [],
            "source": [
                "def is_pinned_version(version: str) -> bool:\n",
                "    \"\"\"\n",
                "    Check if version is properly pinned or uses a range.\n",
                "    Returns False if it looks like semver range (^, ~, >, <, *, x)\n",
                "    \"\"\"\n",
                "    if not version:\n",
                "        return False\n",
                "    # These chars indicate version ranges\n",
                "    bad_chars = [\"^\", \"~\", \">\", \"<\", \"*\", \"x\", \"X\"]\n",
                "    return not any(ch in version for ch in bad_chars)\n",
                "\n",
                "def compute_version_fidelity(sbom: Dict[str, Any]) -> VersionFidelityMetrics:\n",
                "    \"\"\"Analyze version pinning and hash completeness\"\"\"\n",
                "    components = get_components(sbom)\n",
                "    if not components:\n",
                "        return VersionFidelityMetrics(0.0, 0.0, 0.0)\n",
                "\n",
                "    # Version specificity - are versions pinned?\n",
                "    with_version = [c for c in components if c.get(\"version\")]\n",
                "    pinned = [c for c in with_version if is_pinned_version(c.get(\"version\", \"\"))]\n",
                "    version_specificity = len(pinned) / len(with_version) if with_version else 0.0\n",
                "\n",
                "    # Hash completeness - do we have integrity hashes?\n",
                "    with_hash = [c for c in components if c.get(\"hashes\")]\n",
                "    hash_completeness = len(with_hash) / len(components)\n",
                "\n",
                "    # Environment reproducibility = weighted combo of above\n",
                "    env_repro = 0.5 * version_specificity + 0.5 * hash_completeness\n",
                "\n",
                "    return VersionFidelityMetrics(\n",
                "        version_specificity=version_specificity,\n",
                "        hash_completeness=hash_completeness,\n",
                "        env_reproducibility=env_repro,\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4092ea00",
            "metadata": {},
            "source": [
                "## 3. Integrity & Provenance Analysis\n",
                "\n",
                "Evaluates whether we know where our components come from and if they can be verified. This includes:\n",
                "- Supplier information\n",
                "- Cryptographic signatures (if available)\n",
                "- Checking for unauthorized components"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1ea3c898",
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_integrity_provenance(\n",
                "    sbom: Dict[str, Any],\n",
                "    signature_validation: Optional[Dict[str, bool]] = None,\n",
                "    trusted_suppliers: Optional[List[str]] = None,\n",
                ") -> IntegrityProvenanceMetrics:\n",
                "    \"\"\"\n",
                "    Analyze component integrity and provenance.\n",
                "    \n",
                "    Args:\n",
                "        signature_validation: mapping of bom-ref -> bool (was signature valid?)\n",
                "        trusted_suppliers: list of approved supplier names\n",
                "    \"\"\"\n",
                "    components = get_components(sbom)\n",
                "    if not components:\n",
                "        return IntegrityProvenanceMetrics(0.0, 0.0, 0)\n",
                "\n",
                "    # Provenance completeness: do we have supplier info or external references?\n",
                "    prov_ok = 0\n",
                "    for c in components:\n",
                "        supplier = c.get(\"supplier\")\n",
                "        ext_refs = c.get(\"externalReferences\") or []\n",
                "        if supplier or ext_refs:\n",
                "            prov_ok += 1\n",
                "    provenance_completeness = prov_ok / len(components)\n",
                "\n",
                "    # Signature validation - if we have external validation data\n",
                "    if signature_validation:\n",
                "        vals = list(signature_validation.values())\n",
                "        sig_rate = sum(1 for v in vals if v) / len(vals) if vals else 0.0\n",
                "    else:\n",
                "        sig_rate = 0.0  # No signing = 0% validation\n",
                "\n",
                "    # Count unauthorized/untrusted components\n",
                "    unauthorized = 0\n",
                "    if trusted_suppliers:\n",
                "        tset = {t.lower() for t in trusted_suppliers}\n",
                "        for c in components:\n",
                "            supplier = c.get(\"supplier\") or {}\n",
                "            # Supplier might be a dict in CycloneDX\n",
                "            sname = supplier.get(\"name\") if isinstance(supplier, dict) else str(supplier)\n",
                "            if sname and sname.lower() not in tset:\n",
                "                unauthorized += 1\n",
                "\n",
                "    return IntegrityProvenanceMetrics(\n",
                "        provenance_completeness=provenance_completeness,\n",
                "        signature_validation_rate=sig_rate,\n",
                "        unauthorized_component_count=unauthorized,\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1b176ece",
            "metadata": {},
            "source": [
                "## 4. Impact Traceability Analysis\n",
                "\n",
                "Analyzes the dependency graph to understand:\n",
                "- How complete is our dependency mapping?\n",
                "- What's the blast radius if a component gets compromised?\n",
                "- How traceable are dependencies?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6762aa42",
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_impact_traceability(\n",
                "    sbom: Dict[str, Any],\n",
                "    impact_analysis_time_minutes: Optional[float] = None,\n",
                ") -> ImpactTraceabilityMetrics:\n",
                "    \"\"\"\n",
                "    Calculate dependency graph completeness and blast radius.\n",
                "    \n",
                "    - dep_graph_completeness: % of components in the dependency graph\n",
                "    - avg_blast_radius: average number of dependents (downstream) per component\n",
                "    \"\"\"\n",
                "    components = get_components(sbom)\n",
                "    bom_refs = {c.get(\"bom-ref\", c.get(\"name\")) for c in components}\n",
                "    dep_map = get_dependencies_map(sbom)\n",
                "\n",
                "    if not components:\n",
                "        return ImpactTraceabilityMetrics(0.0, impact_analysis_time_minutes, 0.0)\n",
                "\n",
                "    # How many components are represented in the dependency graph?\n",
                "    represented = sum(1 for ref in bom_refs if ref in dep_map)\n",
                "    dep_graph_completeness = represented / len(bom_refs)\n",
                "\n",
                "    # Calculate blast radius - how many things depend on each component?\n",
                "    reverse_dep_counts = {ref: 0 for ref in bom_refs}\n",
                "    for ref, deps in dep_map.items():\n",
                "        for d in deps:\n",
                "            if d in reverse_dep_counts:\n",
                "                reverse_dep_counts[d] += 1\n",
                "\n",
                "    avg_blast_radius = statistics.mean(reverse_dep_counts.values()) if reverse_dep_counts else 0.0\n",
                "\n",
                "    return ImpactTraceabilityMetrics(\n",
                "        dep_graph_completeness=dep_graph_completeness,\n",
                "        impact_analysis_time_minutes=impact_analysis_time_minutes,\n",
                "        avg_blast_radius=avg_blast_radius,\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cb703655",
            "metadata": {},
            "source": [
                "## Main Computation Function\n",
                "\n",
                "Ties everything together to compute all L4 supply chain metrics from an SBOM file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4c55740c",
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_supply_chain_l4_metrics(\n",
                "    sbom_path: Path,\n",
                "    expected_models: Optional[List[str]] = None,\n",
                "    expected_datasets: Optional[List[str]] = None,\n",
                "    expected_plugins: Optional[List[str]] = None,\n",
                "    expected_software: Optional[List[str]] = None,\n",
                "    signature_validation: Optional[Dict[str, bool]] = None,\n",
                "    trusted_suppliers: Optional[List[str]] = None,\n",
                "    vulnerability_records: Optional[List[Dict[str, Any]]] = None,\n",
                "    latest_version_info: Optional[Dict[str, Dict[str, Any]]] = None,\n",
                "    impact_analysis_time_minutes: Optional[float] = None,\n",
                ") -> SupplyChainL4Metrics:\n",
                "    \"\"\"Run all the supply chain analyses and return comprehensive metrics\"\"\"\n",
                "    sbom = load_cyclonedx_sbom(sbom_path)\n",
                "\n",
                "    # Run each analysis\n",
                "    dep_cov = compute_dependency_coverage(\n",
                "        sbom,\n",
                "        expected_models=expected_models,\n",
                "        expected_datasets=expected_datasets,\n",
                "        expected_plugins=expected_plugins,\n",
                "        expected_software_components=expected_software,\n",
                "    )\n",
                "    \n",
                "    ver_fid = compute_version_fidelity(sbom)\n",
                "    \n",
                "    integ_prov = compute_integrity_provenance(\n",
                "        sbom,\n",
                "        signature_validation=signature_validation,\n",
                "        trusted_suppliers=trusted_suppliers,\n",
                "    )\n",
                "    \n",
                "    imp_trace = compute_impact_traceability(\n",
                "        sbom,\n",
                "        impact_analysis_time_minutes=impact_analysis_time_minutes,\n",
                "    )\n",
                "\n",
                "    return SupplyChainL4Metrics(\n",
                "        dependency_coverage=dep_cov,\n",
                "        version_fidelity=ver_fid,\n",
                "        integrity_provenance=integ_prov,\n",
                "        impact_traceability=imp_trace,\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "144a19c9",
            "metadata": {},
            "source": [
                "## Scoring System\n",
                "\n",
                "Now we need to convert our metrics into a 0-100 score. We use thresholds to map percentages to ordinal scores, then combine category scores with weights."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3db69ab5",
            "metadata": {},
            "outputs": [],
            "source": [
                "def score_from_percentage(x: float, thresholds: Tuple[float, float, float]) -> int:\n",
                "    \"\"\"\n",
                "    Map a 0-1 value to ordinal score 0-4 based on thresholds.\n",
                "    Example thresholds: (0.5, 0.75, 0.9) = low, mid, high\n",
                "    \"\"\"\n",
                "    if x is None:\n",
                "        return 0\n",
                "    if x < thresholds[0]:\n",
                "        return 1  # Poor\n",
                "    if x < thresholds[1]:\n",
                "        return 2  # Fair\n",
                "    if x < thresholds[2]:\n",
                "        return 3  # Good\n",
                "    return 4  # Excellent\n",
                "\n",
                "def score_supply_chain_l4(metrics: SupplyChainL4Metrics) -> Dict[str, Any]:\n",
                "    \"\"\"\n",
                "    Calculate final scores from raw metrics.\n",
                "    \n",
                "    Returns:\n",
                "        - overall_score: 0-100\n",
                "        - category_scores: dict of 0-100 scores per category\n",
                "        - raw_metrics: the underlying measurements\n",
                "    \"\"\"\n",
                "    # Thresholds for scoring - adjust based on your org's standards\n",
                "    pct_thresh = (0.5, 0.75, 0.9)  # low, ok, strong\n",
                "    \n",
                "    # Dependency Coverage score\n",
                "    dep_vals = [\n",
                "        metrics.dependency_coverage.software_dependency_coverage,\n",
                "        metrics.dependency_coverage.model_artifact_coverage,\n",
                "        metrics.dependency_coverage.dataset_coverage,\n",
                "        metrics.dependency_coverage.plugin_tool_coverage,\n",
                "    ]\n",
                "    dep_scores = [score_from_percentage(v, pct_thresh) for v in dep_vals]\n",
                "    dep_score = sum(dep_scores) / (4 * 4) * 100  # normalize to 0-100\n",
                "\n",
                "    # Version Fidelity score\n",
                "    vf_vals = [\n",
                "        metrics.version_fidelity.version_specificity,\n",
                "        metrics.version_fidelity.hash_completeness,\n",
                "        metrics.version_fidelity.env_reproducibility,\n",
                "    ]\n",
                "    vf_scores = [score_from_percentage(v, pct_thresh) for v in vf_vals]\n",
                "    vf_score = sum(vf_scores) / (3 * 4) * 100\n",
                "\n",
                "    # Integrity & Provenance score\n",
                "    ip_vals = [\n",
                "        metrics.integrity_provenance.provenance_completeness,\n",
                "        metrics.integrity_provenance.signature_validation_rate,\n",
                "    ]\n",
                "    ip_scores = [score_from_percentage(v, pct_thresh) for v in ip_vals]\n",
                "    ip_base = sum(ip_scores) / (2 * 4) * 100\n",
                "    # Penalty: subtract 5 points for each unauthorized component\n",
                "    ip_score = max(0.0, ip_base - 5.0 * metrics.integrity_provenance.unauthorized_component_count)\n",
                "\n",
                "    # Impact Traceability score\n",
                "    it_vals = [\n",
                "        metrics.impact_traceability.dep_graph_completeness,\n",
                "    ]\n",
                "    it_scores = [score_from_percentage(v, pct_thresh) for v in it_vals]\n",
                "    it_score = sum(it_scores) / (1 * 4) * 100\n",
                "\n",
                "    category_scores = {\n",
                "        \"dependency_coverage\": dep_score,\n",
                "        \"version_fidelity\": vf_score,\n",
                "        \"integrity_provenance\": ip_score,\n",
                "        \"impact_traceability\": it_score,\n",
                "    }\n",
                "    \n",
                "    # Weighted overall score\n",
                "    # These weights represent relative importance - adjust as needed\n",
                "    overall = (category_scores['dependency_coverage'] * 0.35) + \\\n",
                "              (category_scores['version_fidelity'] * 0.30) + \\\n",
                "              (category_scores['integrity_provenance'] * 0.25) + \\\n",
                "              (category_scores['impact_traceability'] * 0.10)\n",
                "\n",
                "    return {\n",
                "        \"overall_score\": overall,\n",
                "        \"category_scores\": category_scores,\n",
                "        \"raw_metrics\": {\n",
                "            \"dependency_coverage\": asdict(metrics.dependency_coverage),\n",
                "            \"version_fidelity\": asdict(metrics.version_fidelity),\n",
                "            \"integrity_provenance\": asdict(metrics.integrity_provenance),\n",
                "            \"impact_traceability\": asdict(metrics.impact_traceability),\n",
                "        },\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "44926941",
            "metadata": {},
            "source": [
                "## Evaluation: DeepSeek Model Environment\n",
                "\n",
                "Let's analyze the DeepSeek model's supply chain first."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "b0fd3410",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Overall L4 Supply-chain score: 58.4375\n",
                        "Category scores: {\n",
                        "  \"dependency_coverage\": 43.75,\n",
                        "  \"version_fidelity\": 58.333333333333336,\n",
                        "  \"integrity_provenance\": 62.5,\n",
                        "  \"impact_traceability\": 100.0\n",
                        "}\n"
                    ]
                }
            ],
            "source": [
                "# DeepSeek SBOM evaluation\n",
                "if __name__ == \"__main__\":\n",
                "    sbom_path = Path(\"../docs/sbom_deepseek_llm_env.json\")  \n",
                "    metrics = compute_supply_chain_l4_metrics(sbom_path)\n",
                "    scored = score_supply_chain_l4(metrics)\n",
                "\n",
                "    print(\"Overall L4 Supply-chain score:\", scored[\"overall_score\"])\n",
                "    print(\"Category scores:\", json.dumps(scored[\"category_scores\"], indent=2))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "50a72222",
            "metadata": {},
            "source": [
                "**DeepSeek Results Analysis:**\n",
                "- Overall score of 58.4% is moderate - room for improvement\n",
                "- Dependency coverage at 43.75% is concerning - we're missing a lot of documented dependencies\n",
                "- Perfect impact traceability (100%) is great - we have full dependency graph\n",
                "- Integrity/provenance at 62.5% suggests we have decent supplier info but could be better\n",
                "\n",
                "## Evaluation: Llama Model Environment\n",
                "\n",
                "Now let's check the Llama model for comparison."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "id": "a2b29da1",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Overall L4 Supply-chain score: 46.5625\n",
                        "Category scores: {\n",
                        "  \"dependency_coverage\": 43.75,\n",
                        "  \"version_fidelity\": 75.0,\n",
                        "  \"integrity_provenance\": 25.0,\n",
                        "  \"impact_traceability\": 25.0\n",
                        "}\n"
                    ]
                }
            ],
            "source": [
                "# Llama SBOM evaluation\n",
                "if __name__ == \"__main__\":\n",
                "    sbom_path = Path(\"../docs/sbom_llama_env.json\")  \n",
                "    \n",
                "    metrics = compute_supply_chain_l4_metrics(sbom_path)\n",
                "    scored = score_supply_chain_l4(metrics)\n",
                "\n",
                "    print(\"Overall L4 Supply-chain score:\", scored[\"overall_score\"])\n",
                "    print(\"Category scores:\", json.dumps(scored[\"category_scores\"], indent=2))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
